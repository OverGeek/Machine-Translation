{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KX19a-At-av5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc9a717c-de7f-4129-d9fc-7d35e0d630ed"
      },
      "cell_type": "code",
      "source": [
        "!unzip 'Machine Translation.zip'"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open Machine Translation.zip, Machine Translation.zip.zip or Machine Translation.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A02rBRXy-r10",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "85089aaa-b2fe-473c-f089-39c860c7855a"
      },
      "cell_type": "code",
      "source": [
        "cd 'Machine Translation'"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'Machine Translation'\n",
            "/content/Machine Translation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4bWVqEKAQxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f1424fc8-8813-4093-c3f4-4f68785b05f4"
      },
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from faker) (2.5.3)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from faker) (1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from faker) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ems1aVK7-2ui",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CREATING A DATASET FOR MACHINE TRANSLATION**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2VtxaCr8_L4C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "\n",
        "fake = Faker()\n",
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "847AZS0H_I4w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Loads some fake dates \n",
        "   :returns: tuple containing human readable string, machine readable string, and date object\n",
        "\"\"\"\n",
        "\n",
        "def load_date():\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "        \n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JcE5vixQAutx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2687466-20fb-4006-eb18-7e70fcbbe290"
      },
      "cell_type": "code",
      "source": [
        "load_date()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('monday september 18 2017', '2017-09-18', datetime.date(2017, 9, 18))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "metadata": {
        "id": "IUolDrpOAwVa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Loads a dataset with m examples and vocabularies\n",
        "    m: the number of examples to generate\n",
        "\"\"\"\n",
        "def load_dataset(m):\n",
        "    \n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    Tx = 30\n",
        "    \n",
        "\n",
        "    for i in tqdm(range(m)):   # tqdm just generates a progress bar\n",
        "        h, m, _ = load_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, m))\n",
        "            human_vocab.update(tuple(h))   # tuple('string') = ('s', 't,', 'r', 'i', 'n', 'g')\n",
        "            machine_vocab.update(tuple(m))  \n",
        "         \n",
        "    print(human_vocab)\n",
        "    \n",
        "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'],  #sorted will create a list of the set in sorted order\n",
        "                     list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
        "    machine = {v:k for k,v in inv_machine.items()}\n",
        " \n",
        "    return dataset, human, machine, inv_machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q_udD7taBf2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1023
        },
        "outputId": "a8ed6b0e-2f08-4b96-dccc-58d5f04821f7"
      },
      "cell_type": "code",
      "source": [
        "load_dataset(5)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 3309.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'9', 'g', '6', '2', 'h', 't', 'c', 'n', 'm', 'p', '5', '1', 's', 'a', 'b', 'l', 'i', 'e', '3', '7', 'd', 'r', 'u', ' ', '0', 'y', 'o'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('sunday october 25 1992', '1992-10-25'),\n",
              "  ('20 03 07', '2007-03-20'),\n",
              "  ('sunday may 9 1976', '1976-05-09'),\n",
              "  ('tuesday august 22 1972', '1972-08-22'),\n",
              "  ('thursday april 12 1973', '1973-04-12')],\n",
              " {' ': 0,\n",
              "  '0': 1,\n",
              "  '1': 2,\n",
              "  '2': 3,\n",
              "  '3': 4,\n",
              "  '5': 5,\n",
              "  '6': 6,\n",
              "  '7': 7,\n",
              "  '9': 8,\n",
              "  '<pad>': 28,\n",
              "  '<unk>': 27,\n",
              "  'a': 9,\n",
              "  'b': 10,\n",
              "  'c': 11,\n",
              "  'd': 12,\n",
              "  'e': 13,\n",
              "  'g': 14,\n",
              "  'h': 15,\n",
              "  'i': 16,\n",
              "  'l': 17,\n",
              "  'm': 18,\n",
              "  'n': 19,\n",
              "  'o': 20,\n",
              "  'p': 21,\n",
              "  'r': 22,\n",
              "  's': 23,\n",
              "  't': 24,\n",
              "  'u': 25,\n",
              "  'y': 26},\n",
              " {'-': 0,\n",
              "  '0': 1,\n",
              "  '1': 2,\n",
              "  '2': 3,\n",
              "  '3': 4,\n",
              "  '4': 5,\n",
              "  '5': 6,\n",
              "  '6': 7,\n",
              "  '7': 8,\n",
              "  '8': 9,\n",
              "  '9': 10},\n",
              " {0: '-',\n",
              "  1: '0',\n",
              "  2: '1',\n",
              "  3: '2',\n",
              "  4: '3',\n",
              "  5: '4',\n",
              "  6: '5',\n",
              "  7: '6',\n",
              "  8: '7',\n",
              "  9: '8',\n",
              "  10: '9'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "metadata": {
        "id": "Kgbm8eFnCUnT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1d80360d-4152-4e5f-9acb-a7b38f69aa04"
      },
      "cell_type": "code",
      "source": [
        "# creating a dataset of 10000 training entries\n",
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 17124.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'9', '.', 'g', 'j', '6', '2', 'h', 't', 'c', 'n', 'm', 'p', '5', '1', 'f', 's', 'a', 'b', '/', 'l', 'i', 'e', 'v', 'w', '3', '7', 'd', 'r', 'u', ' ', '4', '0', '8', 'y', 'o'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "WeDvieVaES8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "aa3ef54b-9308-4b29-f2f1-0b9526abadda"
      },
      "cell_type": "code",
      "source": [
        "dataset[:10]"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2 jan 1971', '1971-01-02'),\n",
              " ('18 aug 1980', '1980-08-18'),\n",
              " ('december 11 1982', '1982-12-11'),\n",
              " ('saturday july 28 2007', '2007-07-28'),\n",
              " ('wednesday march 3 1999', '1999-03-03'),\n",
              " ('wednesday september 26 1979', '1979-09-26'),\n",
              " ('18 march 1976', '1976-03-18'),\n",
              " ('30 04 77', '1977-04-30'),\n",
              " ('sunday december 25 1988', '1988-12-25'),\n",
              " ('saturday april 20 1996', '1996-04-20')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "metadata": {
        "id": "pVoKQ-rAEp-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**PRE-PROCESSING DATA, MAPPING STRING TO LIST OF INTEGERS**"
      ]
    },
    {
      "metadata": {
        "id": "HvpNXVziE643",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "def string_to_int(string, length, vocab):\n",
        "    \"\"\"\n",
        "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
        "    input string's characters in the \"vocab\"\n",
        "    \n",
        "    Arguments:\n",
        "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
        "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
        "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
        "    \n",
        "    Returns:\n",
        "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
        "    \"\"\"\n",
        "    \n",
        "    #make lower to standardize\n",
        "    string = string.lower()\n",
        "    string = string.replace(',','')\n",
        "    \n",
        "    if len(string) > length:\n",
        "        string = string[:length]\n",
        "        \n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
        "    #print(rep)\n",
        "    \n",
        "    if len(string) < length:\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\n",
        "    \n",
        "    #print (rep)\n",
        "    return rep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eF5ZuNZXHVPI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def int_to_string(ints, inv_vocab):\n",
        "    \"\"\"\n",
        "    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n",
        "    \n",
        "    Arguments:\n",
        "    ints -- list of integers representing indexes in the machine's vocabulary\n",
        "    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n",
        "    \n",
        "    Returns:\n",
        "    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n",
        "    \"\"\"\n",
        "    \n",
        "    l = [inv_vocab[i] for i in ints]\n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SCuFav9YHbpJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
        "    \n",
        "    X, Y = zip(*dataset)\n",
        "    \n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
        "    Y = np.array([string_to_int(t, Ty, machine_vocab) for t in Y])\n",
        "    \n",
        "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "    return X, Y, Xoh, Yoh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Tclc732IV-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4470345e-e155-432c-e503-34d3bf81c81e"
      },
      "cell_type": "code",
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QL7KiEjOJN9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BUILDING THE ATTENTION MECHANISM TO CALCULATE CONTEXT OF EACH TIME-STEP OF PRE-ATTENTION BI-LSTM**"
      ]
    },
    {
      "metadata": {
        "id": "5nBNxeeUIcxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import RepeatVector, Dense, Activation, Concatenate, Dense, Dot, LSTM\n",
        "import keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-pief_QfLLSU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JCoqqpeSJ6Zg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = 'tanh')\n",
        "densor2 = Dense(1, activation = 'relu')\n",
        "activator = Activation(softmax, name = 'attention_weights_alpha')  # Custom softmax to calculate about axis = 1\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UTkwX2LZLm0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Calculates context value for one time-step\n",
        "\n",
        "def one_step_attention(a, s_prev):\n",
        "  \"\"\"Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "  \"\"\"\n",
        "  \n",
        "  s_prev = repeator(s_prev)\n",
        "  concat = concatenator([a, s_prev])\n",
        "  e = densor1(concat)\n",
        "  energies = densor2(e)\n",
        "  alphas = activator(energies)\n",
        "  context = dotor([alphas, a])\n",
        "  \n",
        "  return context  # shape of context is (m, 2*n_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0OOIvUfNMt2P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DEFINING LAYERS OF POST-LSTM MODEL**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BYcYi3HsMycs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_a = 32\n",
        "n_s = 64\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation = 'softmax')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBIOoAX-Vj_C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**MAKING THE MODEL**"
      ]
    },
    {
      "metadata": {
        "id": "sMP8r8PLVmwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, Input\n",
        "from keras.models import Model\n",
        "\n",
        "def model(Tx, Ty, n_a, n_s, human_vocab, machine_vocab):\n",
        "  X = Input((Tx, human_vocab))\n",
        "  s0 = Input((n_s,), name = 's0')\n",
        "  c0 = Input((n_s,), name = 'c0')\n",
        "  s = s0\n",
        "  c = c0\n",
        "  \n",
        "  outputs = []\n",
        "  \n",
        "  a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\n",
        "  \n",
        "  for t in range(Ty):\n",
        "    context = one_step_attention(a, s)\n",
        "    s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
        "    out = output_layer(s)\n",
        "    outputs.append(out)\n",
        "    \n",
        "  model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "os8QEKkIX9Of",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJejhBlpYUw6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2448
        },
        "outputId": "d231cf55-d643-47cf-8e1b-06bf3677806a"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_8 (Bidirectional) (None, 30, 64)       17920       input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_5 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_13[0][0]                    \n",
            "                                                                 lstm_13[1][0]                    \n",
            "                                                                 lstm_13[2][0]                    \n",
            "                                                                 lstm_13[3][0]                    \n",
            "                                                                 lstm_13[4][0]                    \n",
            "                                                                 lstm_13[5][0]                    \n",
            "                                                                 lstm_13[6][0]                    \n",
            "                                                                 lstm_13[7][0]                    \n",
            "                                                                 lstm_13[8][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 30, 128)      0           bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[0][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[1][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[2][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[3][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[4][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[5][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[6][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[7][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[8][0]            \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 repeat_vector_5[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 30, 10)       1290        concatenate_5[0][0]              \n",
            "                                                                 concatenate_5[1][0]              \n",
            "                                                                 concatenate_5[2][0]              \n",
            "                                                                 concatenate_5[3][0]              \n",
            "                                                                 concatenate_5[4][0]              \n",
            "                                                                 concatenate_5[5][0]              \n",
            "                                                                 concatenate_5[6][0]              \n",
            "                                                                 concatenate_5[7][0]              \n",
            "                                                                 concatenate_5[8][0]              \n",
            "                                                                 concatenate_5[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 30, 1)        11          dense_14[0][0]                   \n",
            "                                                                 dense_14[1][0]                   \n",
            "                                                                 dense_14[2][0]                   \n",
            "                                                                 dense_14[3][0]                   \n",
            "                                                                 dense_14[4][0]                   \n",
            "                                                                 dense_14[5][0]                   \n",
            "                                                                 dense_14[6][0]                   \n",
            "                                                                 dense_14[7][0]                   \n",
            "                                                                 dense_14[8][0]                   \n",
            "                                                                 dense_14[9][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights_alpha (Activa (None, 30, 1)        0           dense_15[0][0]                   \n",
            "                                                                 dense_15[1][0]                   \n",
            "                                                                 dense_15[2][0]                   \n",
            "                                                                 dense_15[3][0]                   \n",
            "                                                                 dense_15[4][0]                   \n",
            "                                                                 dense_15[5][0]                   \n",
            "                                                                 dense_15[6][0]                   \n",
            "                                                                 dense_15[7][0]                   \n",
            "                                                                 dense_15[8][0]                   \n",
            "                                                                 dense_15[9][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_5 (Dot)                     (None, 1, 64)        0           attention_weights_alpha[0][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[1][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[2][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[3][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[4][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[5][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[6][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[7][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[8][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "                                                                 attention_weights_alpha[9][0]    \n",
            "                                                                 bidirectional_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  [(None, 64), (None,  33024       dot_5[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_5[1][0]                      \n",
            "                                                                 lstm_13[0][0]                    \n",
            "                                                                 lstm_13[0][2]                    \n",
            "                                                                 dot_5[2][0]                      \n",
            "                                                                 lstm_13[1][0]                    \n",
            "                                                                 lstm_13[1][2]                    \n",
            "                                                                 dot_5[3][0]                      \n",
            "                                                                 lstm_13[2][0]                    \n",
            "                                                                 lstm_13[2][2]                    \n",
            "                                                                 dot_5[4][0]                      \n",
            "                                                                 lstm_13[3][0]                    \n",
            "                                                                 lstm_13[3][2]                    \n",
            "                                                                 dot_5[5][0]                      \n",
            "                                                                 lstm_13[4][0]                    \n",
            "                                                                 lstm_13[4][2]                    \n",
            "                                                                 dot_5[6][0]                      \n",
            "                                                                 lstm_13[5][0]                    \n",
            "                                                                 lstm_13[5][2]                    \n",
            "                                                                 dot_5[7][0]                      \n",
            "                                                                 lstm_13[6][0]                    \n",
            "                                                                 lstm_13[6][2]                    \n",
            "                                                                 dot_5[8][0]                      \n",
            "                                                                 lstm_13[7][0]                    \n",
            "                                                                 lstm_13[7][2]                    \n",
            "                                                                 dot_5[9][0]                      \n",
            "                                                                 lstm_13[8][0]                    \n",
            "                                                                 lstm_13[8][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 11)           715         lstm_13[0][0]                    \n",
            "                                                                 lstm_13[1][0]                    \n",
            "                                                                 lstm_13[2][0]                    \n",
            "                                                                 lstm_13[3][0]                    \n",
            "                                                                 lstm_13[4][0]                    \n",
            "                                                                 lstm_13[5][0]                    \n",
            "                                                                 lstm_13[6][0]                    \n",
            "                                                                 lstm_13[7][0]                    \n",
            "                                                                 lstm_13[8][0]                    \n",
            "                                                                 lstm_13[9][0]                    \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TJG7xKp6aUeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "opt = Adam(lr = 0.005, beta_1 = 0.9, beta_2 = 0.999, decay = 0.01)\n",
        "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3Zo43RCi3Zr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "target = list(Yoh.swapaxes(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7cw6sqVdjNCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3454
        },
        "outputId": "c69df74b-5e8d-448b-d7a4-59d816ece244"
      },
      "cell_type": "code",
      "source": [
        "model.fit([Xoh, s0, c0], target, epochs = 100, batch_size = 128)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10000/10000 [==============================] - 28s 3ms/step - loss: 18.7905 - dense_16_loss: 2.6185 - dense_16_acc: 0.2969 - dense_16_acc_1: 0.5880 - dense_16_acc_2: 0.2545 - dense_16_acc_3: 0.0549 - dense_16_acc_4: 0.9238 - dense_16_acc_5: 0.2063 - dense_16_acc_6: 0.0144 - dense_16_acc_7: 0.9213 - dense_16_acc_8: 0.2045 - dense_16_acc_9: 0.0934\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 10.4376 - dense_16_loss: 2.2660 - dense_16_acc: 0.9406 - dense_16_acc_1: 0.9507 - dense_16_acc_2: 0.4632 - dense_16_acc_3: 0.1498 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.7929 - dense_16_acc_6: 0.2010 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.4459 - dense_16_acc_9: 0.1457\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 8.2700 - dense_16_loss: 2.0664 - dense_16_acc: 0.9730 - dense_16_acc_1: 0.9730 - dense_16_acc_2: 0.6186 - dense_16_acc_3: 0.2555 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9065 - dense_16_acc_6: 0.4376 - dense_16_acc_7: 0.9998 - dense_16_acc_8: 0.5691 - dense_16_acc_9: 0.2369\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 6.5272 - dense_16_loss: 1.7811 - dense_16_acc: 0.9776 - dense_16_acc_1: 0.9786 - dense_16_acc_2: 0.7712 - dense_16_acc_3: 0.4792 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9544 - dense_16_acc_6: 0.6446 - dense_16_acc_7: 0.9999 - dense_16_acc_8: 0.6695 - dense_16_acc_9: 0.3496\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 4.8066 - dense_16_loss: 1.3556 - dense_16_acc: 0.9783 - dense_16_acc_1: 0.9815 - dense_16_acc_2: 0.8593 - dense_16_acc_3: 0.7252 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9702 - dense_16_acc_6: 0.7744 - dense_16_acc_7: 0.9998 - dense_16_acc_8: 0.7295 - dense_16_acc_9: 0.5359\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 3.4379 - dense_16_loss: 0.8753 - dense_16_acc: 0.9786 - dense_16_acc_1: 0.9841 - dense_16_acc_2: 0.8934 - dense_16_acc_3: 0.8698 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9735 - dense_16_acc_6: 0.8307 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.7734 - dense_16_acc_9: 0.7265\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 2.5188 - dense_16_loss: 0.5500 - dense_16_acc: 0.9801 - dense_16_acc_1: 0.9878 - dense_16_acc_2: 0.9272 - dense_16_acc_3: 0.9408 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9747 - dense_16_acc_6: 0.8639 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.8163 - dense_16_acc_9: 0.8398\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 1.9720 - dense_16_loss: 0.3891 - dense_16_acc: 0.9806 - dense_16_acc_1: 0.9894 - dense_16_acc_2: 0.9557 - dense_16_acc_3: 0.9736 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9742 - dense_16_acc_6: 0.8813 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.8468 - dense_16_acc_9: 0.8908\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 1.6197 - dense_16_loss: 0.3016 - dense_16_acc: 0.9832 - dense_16_acc_1: 0.9918 - dense_16_acc_2: 0.9743 - dense_16_acc_3: 0.9862 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9761 - dense_16_acc_6: 0.8938 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.8650 - dense_16_acc_9: 0.9188\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 1.3295 - dense_16_loss: 0.2372 - dense_16_acc: 0.9844 - dense_16_acc_1: 0.9934 - dense_16_acc_2: 0.9825 - dense_16_acc_3: 0.9970 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9759 - dense_16_acc_6: 0.9067 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.8944 - dense_16_acc_9: 0.9388\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 1.1099 - dense_16_loss: 0.1896 - dense_16_acc: 0.9874 - dense_16_acc_1: 0.9958 - dense_16_acc_2: 0.9895 - dense_16_acc_3: 0.9991 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9771 - dense_16_acc_6: 0.9179 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9159 - dense_16_acc_9: 0.9534\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.9289 - dense_16_loss: 0.1511 - dense_16_acc: 0.9905 - dense_16_acc_1: 0.9967 - dense_16_acc_2: 0.9920 - dense_16_acc_3: 0.9996 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9779 - dense_16_acc_6: 0.9267 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9388 - dense_16_acc_9: 0.9703\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.7894 - dense_16_loss: 0.1215 - dense_16_acc: 0.9921 - dense_16_acc_1: 0.9973 - dense_16_acc_2: 0.9949 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9795 - dense_16_acc_6: 0.9335 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9519 - dense_16_acc_9: 0.9788\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.6803 - dense_16_loss: 0.1004 - dense_16_acc: 0.9932 - dense_16_acc_1: 0.9981 - dense_16_acc_2: 0.9966 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9802 - dense_16_acc_6: 0.9382 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9634 - dense_16_acc_9: 0.9855\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.5951 - dense_16_loss: 0.0849 - dense_16_acc: 0.9951 - dense_16_acc_1: 0.9988 - dense_16_acc_2: 0.9972 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9814 - dense_16_acc_6: 0.9441 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9714 - dense_16_acc_9: 0.9898\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.5265 - dense_16_loss: 0.0730 - dense_16_acc: 0.9962 - dense_16_acc_1: 0.9996 - dense_16_acc_2: 0.9980 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9818 - dense_16_acc_6: 0.9478 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9784 - dense_16_acc_9: 0.9920\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.4708 - dense_16_loss: 0.0630 - dense_16_acc: 0.9968 - dense_16_acc_1: 0.9995 - dense_16_acc_2: 0.9985 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9827 - dense_16_acc_6: 0.9529 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9833 - dense_16_acc_9: 0.9936\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.4220 - dense_16_loss: 0.0542 - dense_16_acc: 0.9975 - dense_16_acc_1: 0.9999 - dense_16_acc_2: 0.9992 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9830 - dense_16_acc_6: 0.9571 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9873 - dense_16_acc_9: 0.9950\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.3856 - dense_16_loss: 0.0479 - dense_16_acc: 0.9977 - dense_16_acc_1: 0.9999 - dense_16_acc_2: 0.9992 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9830 - dense_16_acc_6: 0.9585 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9902 - dense_16_acc_9: 0.9959\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.3531 - dense_16_loss: 0.0429 - dense_16_acc: 0.9985 - dense_16_acc_1: 0.9999 - dense_16_acc_2: 0.9994 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9844 - dense_16_acc_6: 0.9616 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9921 - dense_16_acc_9: 0.9969\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.3230 - dense_16_loss: 0.0381 - dense_16_acc: 0.9988 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9995 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9843 - dense_16_acc_6: 0.9619 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9939 - dense_16_acc_9: 0.9972\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.2985 - dense_16_loss: 0.0338 - dense_16_acc: 0.9988 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9996 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9856 - dense_16_acc_6: 0.9662 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9960 - dense_16_acc_9: 0.9981\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.2799 - dense_16_loss: 0.0310 - dense_16_acc: 0.9994 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9997 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9857 - dense_16_acc_6: 0.9673 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9960 - dense_16_acc_9: 0.9983\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.2605 - dense_16_loss: 0.0281 - dense_16_acc: 0.9993 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9996 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9861 - dense_16_acc_6: 0.9695 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9973 - dense_16_acc_9: 0.9982\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.2441 - dense_16_loss: 0.0255 - dense_16_acc: 0.9993 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9996 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9871 - dense_16_acc_6: 0.9727 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9981 - dense_16_acc_9: 0.9987\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.2306 - dense_16_loss: 0.0237 - dense_16_acc: 0.9995 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9997 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9869 - dense_16_acc_6: 0.9743 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9979 - dense_16_acc_9: 0.9990\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.2176 - dense_16_loss: 0.0216 - dense_16_acc: 0.9994 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9998 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9868 - dense_16_acc_6: 0.9743 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9984 - dense_16_acc_9: 0.9990\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.2068 - dense_16_loss: 0.0200 - dense_16_acc: 0.9995 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9998 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9864 - dense_16_acc_6: 0.9759 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9991 - dense_16_acc_9: 0.9994\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1960 - dense_16_loss: 0.0187 - dense_16_acc: 0.9995 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9997 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9876 - dense_16_acc_6: 0.9780 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9991 - dense_16_acc_9: 0.9994\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1865 - dense_16_loss: 0.0173 - dense_16_acc: 0.9996 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9998 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9879 - dense_16_acc_6: 0.9783 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9994 - dense_16_acc_9: 0.9996\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1774 - dense_16_loss: 0.0159 - dense_16_acc: 0.9997 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9998 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9871 - dense_16_acc_6: 0.9801 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9994 - dense_16_acc_9: 0.9998\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1692 - dense_16_loss: 0.0149 - dense_16_acc: 0.9998 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9999 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9886 - dense_16_acc_6: 0.9811 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9997 - dense_16_acc_9: 0.9998\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1622 - dense_16_loss: 0.0142 - dense_16_acc: 0.9997 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9998 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9885 - dense_16_acc_6: 0.9820 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9998 - dense_16_acc_9: 0.9998\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1561 - dense_16_loss: 0.0130 - dense_16_acc: 0.9997 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9999 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9884 - dense_16_acc_6: 0.9827 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9998 - dense_16_acc_9: 1.0000\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1510 - dense_16_loss: 0.0124 - dense_16_acc: 0.9998 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9999 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9892 - dense_16_acc_6: 0.9841 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9997 - dense_16_acc_9: 1.0000\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1431 - dense_16_loss: 0.0115 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9999 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9895 - dense_16_acc_6: 0.9850 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9999 - dense_16_acc_9: 1.0000\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1383 - dense_16_loss: 0.0110 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9998 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9891 - dense_16_acc_6: 0.9856 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 0.9999\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1328 - dense_16_loss: 0.0103 - dense_16_acc: 0.9997 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9999 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9893 - dense_16_acc_6: 0.9862 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1278 - dense_16_loss: 0.0098 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 0.9999 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9901 - dense_16_acc_6: 0.9873 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9998 - dense_16_acc_9: 0.9999\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1233 - dense_16_loss: 0.0092 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9897 - dense_16_acc_6: 0.9877 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9999 - dense_16_acc_9: 1.0000\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1196 - dense_16_loss: 0.0088 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9903 - dense_16_acc_6: 0.9879 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1154 - dense_16_loss: 0.0084 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9898 - dense_16_acc_6: 0.9890 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1110 - dense_16_loss: 0.0079 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9902 - dense_16_acc_6: 0.9898 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1089 - dense_16_loss: 0.0075 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9895 - dense_16_acc_6: 0.9891 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1041 - dense_16_loss: 0.0073 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9905 - dense_16_acc_6: 0.9901 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.1011 - dense_16_loss: 0.0071 - dense_16_acc: 0.9999 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9909 - dense_16_acc_6: 0.9902 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0982 - dense_16_loss: 0.0068 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9903 - dense_16_acc_6: 0.9909 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 0.9999 - dense_16_acc_9: 1.0000\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0954 - dense_16_loss: 0.0065 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9909 - dense_16_acc_6: 0.9917 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0919 - dense_16_loss: 0.0061 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9910 - dense_16_acc_6: 0.9921 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0893 - dense_16_loss: 0.0059 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9913 - dense_16_acc_6: 0.9922 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0861 - dense_16_loss: 0.0056 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9912 - dense_16_acc_6: 0.9930 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0841 - dense_16_loss: 0.0055 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9914 - dense_16_acc_6: 0.9934 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0813 - dense_16_loss: 0.0053 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9912 - dense_16_acc_6: 0.9939 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0788 - dense_16_loss: 0.0050 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9919 - dense_16_acc_6: 0.9945 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0770 - dense_16_loss: 0.0050 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9910 - dense_16_acc_6: 0.9944 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0745 - dense_16_loss: 0.0047 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9919 - dense_16_acc_6: 0.9944 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0732 - dense_16_loss: 0.0046 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9918 - dense_16_acc_6: 0.9947 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0703 - dense_16_loss: 0.0044 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9924 - dense_16_acc_6: 0.9954 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0684 - dense_16_loss: 0.0042 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9921 - dense_16_acc_6: 0.9956 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0664 - dense_16_loss: 0.0042 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9924 - dense_16_acc_6: 0.9959 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0644 - dense_16_loss: 0.0039 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9926 - dense_16_acc_6: 0.9960 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0634 - dense_16_loss: 0.0040 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9931 - dense_16_acc_6: 0.9963 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0609 - dense_16_loss: 0.0038 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9930 - dense_16_acc_6: 0.9962 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0590 - dense_16_loss: 0.0036 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9935 - dense_16_acc_6: 0.9967 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0576 - dense_16_loss: 0.0035 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9936 - dense_16_acc_6: 0.9969 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0562 - dense_16_loss: 0.0034 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9935 - dense_16_acc_6: 0.9975 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0545 - dense_16_loss: 0.0033 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9936 - dense_16_acc_6: 0.9973 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0528 - dense_16_loss: 0.0032 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9940 - dense_16_acc_6: 0.9973 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0517 - dense_16_loss: 0.0032 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9940 - dense_16_acc_6: 0.9973 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0501 - dense_16_loss: 0.0030 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9940 - dense_16_acc_6: 0.9980 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0484 - dense_16_loss: 0.0030 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9951 - dense_16_acc_6: 0.9977 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0477 - dense_16_loss: 0.0028 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9947 - dense_16_acc_6: 0.9977 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0457 - dense_16_loss: 0.0027 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9944 - dense_16_acc_6: 0.9981 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0450 - dense_16_loss: 0.0028 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9950 - dense_16_acc_6: 0.9981 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0432 - dense_16_loss: 0.0026 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9951 - dense_16_acc_6: 0.9985 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0419 - dense_16_loss: 0.0025 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9951 - dense_16_acc_6: 0.9982 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0410 - dense_16_loss: 0.0025 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9951 - dense_16_acc_6: 0.9986 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0395 - dense_16_loss: 0.0024 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9956 - dense_16_acc_6: 0.9983 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0382 - dense_16_loss: 0.0023 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9958 - dense_16_acc_6: 0.9988 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0390 - dense_16_loss: 0.0025 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9959 - dense_16_acc_6: 0.9988 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 0.9998\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0360 - dense_16_loss: 0.0022 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9965 - dense_16_acc_6: 0.9990 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0354 - dense_16_loss: 0.0021 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9964 - dense_16_acc_6: 0.9990 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0340 - dense_16_loss: 0.0020 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9963 - dense_16_acc_6: 0.9991 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0331 - dense_16_loss: 0.0020 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9966 - dense_16_acc_6: 0.9993 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0322 - dense_16_loss: 0.0020 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9966 - dense_16_acc_6: 0.9990 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0313 - dense_16_loss: 0.0019 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9969 - dense_16_acc_6: 0.9993 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0303 - dense_16_loss: 0.0019 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9968 - dense_16_acc_6: 0.9993 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0297 - dense_16_loss: 0.0019 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9965 - dense_16_acc_6: 0.9995 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0288 - dense_16_loss: 0.0018 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9968 - dense_16_acc_6: 0.9992 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0281 - dense_16_loss: 0.0017 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9972 - dense_16_acc_6: 0.9993 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0275 - dense_16_loss: 0.0018 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9974 - dense_16_acc_6: 0.9995 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0265 - dense_16_loss: 0.0016 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9973 - dense_16_acc_6: 0.9996 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0257 - dense_16_loss: 0.0016 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9974 - dense_16_acc_6: 0.9996 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0250 - dense_16_loss: 0.0016 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9975 - dense_16_acc_6: 0.9994 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0242 - dense_16_loss: 0.0015 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9976 - dense_16_acc_6: 0.9996 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0238 - dense_16_loss: 0.0015 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9977 - dense_16_acc_6: 0.9995 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0230 - dense_16_loss: 0.0015 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9978 - dense_16_acc_6: 0.9996 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0223 - dense_16_loss: 0.0015 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9979 - dense_16_acc_6: 0.9995 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0220 - dense_16_loss: 0.0014 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9976 - dense_16_acc_6: 0.9997 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0212 - dense_16_loss: 0.0014 - dense_16_acc: 1.0000 - dense_16_acc_1: 1.0000 - dense_16_acc_2: 1.0000 - dense_16_acc_3: 1.0000 - dense_16_acc_4: 1.0000 - dense_16_acc_5: 0.9981 - dense_16_acc_6: 0.9996 - dense_16_acc_7: 1.0000 - dense_16_acc_8: 1.0000 - dense_16_acc_9: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa567aee80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "metadata": {
        "id": "IfeJ14nDqpOH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights('weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6yDmDC9Ik8uF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**TESTING THE MODEL WITH EXAMPLES**"
      ]
    },
    {
      "metadata": {
        "id": "fIVWNV5tjbr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "296bce82-bf67-43a6-b437-a9c12b7faedc"
      },
      "cell_type": "code",
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "for example in EXAMPLES:\n",
        "  source = string_to_int(example, Tx, human_vocab)\n",
        "  source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).reshape((1, Tx, len(human_vocab)))\n",
        "  prediction = model.predict([source, np.zeros((1, n_s)), np.zeros((1, n_s))])\n",
        "  prediction = np.array(np.argmax(prediction, axis = -1))\n",
        "  output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "  \n",
        "  print('source: ', example)\n",
        "  print('output: ', ''.join(output))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:  3 May 1979\n",
            "output:  1979-05-03\n",
            "source:  5 April 09\n",
            "output:  2009-04-05\n",
            "source:  21th of August 2016\n",
            "output:  2016-08-00\n",
            "source:  Tue 10 Jul 2007\n",
            "output:  2007-07-10\n",
            "source:  Saturday May 9 2018\n",
            "output:  2018-05-09\n",
            "source:  March 3 2001\n",
            "output:  2001-03-03\n",
            "source:  March 3rd 2001\n",
            "output:  2001-03-00\n",
            "source:  1 March 2001\n",
            "output:  2001-03-01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nkmi0wjJua1S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**VISUALISING ATTENTION WEIGHTS ALPHAS**"
      ]
    },
    {
      "metadata": {
        "id": "dm0pG6oXuhZf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_attention_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 6, Tx = 30, Ty = 10):\n",
        "    \"\"\"\n",
        "    Plot the attention map.\n",
        "  \n",
        "    \"\"\"\n",
        "    attention_map = np.zeros((10, 30))\n",
        "    Ty, Tx = attention_map.shape\n",
        "    \n",
        "    s0 = np.zeros((1, n_s))\n",
        "    c0 = np.zeros((1, n_s))\n",
        "    layer = model.layers[num]\n",
        "\n",
        "    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n",
        "    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
        "\n",
        "    f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
        "    r = f([encoded, s0, c0])\n",
        "    \n",
        "    for t in range(Ty):\n",
        "        for t_prime in range(Tx):\n",
        "            attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
        "\n",
        "    # Normalize attention map\n",
        "#     row_max = attention_map.max(axis=1)\n",
        "#     attention_map = attention_map / row_max[:, None]\n",
        "\n",
        "    prediction = model.predict([encoded, s0, c0])\n",
        "    \n",
        "    predicted_text = []\n",
        "    for i in range(len(prediction)):\n",
        "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
        "        \n",
        "    predicted_text = list(predicted_text)\n",
        "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
        "    text_ = list(text)\n",
        "    \n",
        "    # get the lengths of the string\n",
        "    input_length = len(text)\n",
        "    output_length = Ty\n",
        "    \n",
        "    # Plot the attention_map\n",
        "    plt.clf()\n",
        "    f = plt.figure(figsize=(8, 8.5))\n",
        "    ax = f.add_subplot(1, 1, 1)\n",
        "\n",
        "    # add image\n",
        "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "    # add colorbar\n",
        "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
        "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
        "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
        "\n",
        "    # add labels\n",
        "    ax.set_yticks(range(output_length))\n",
        "    ax.set_yticklabels(predicted_text[:output_length])\n",
        "\n",
        "    ax.set_xticks(range(input_length))\n",
        "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
        "\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "\n",
        "    # add grid and legend\n",
        "    ax.grid()\n",
        "\n",
        "    #f.show()\n",
        "    \n",
        "    return attention_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ooTJV38uiMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "37ba63ec-0311-48c6-9069-af8e84d299d9"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ffa50069828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAGpCAYAAAC3X/5IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ffa50069278>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "-dMF4Zzduj2V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}